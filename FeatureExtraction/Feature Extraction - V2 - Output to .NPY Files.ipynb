{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# edf libraries\n",
    "import pyedflib\n",
    "import mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Feature Extraction from EEG Data - Output to .NPY files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " This notebook reads raw EEG data from EDF files, selects as subset of channels and extracts features for each\n",
       "            of those channels. Data is saved to numpy (.NPY) files for later use. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please note that patient s07 has been removed due to poor quality. The first and last 120s of data have been removed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "No noise reduction has been applied."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(\"# Feature Extraction from EEG Data - Output to .NPY files\")\n",
    "printmd(\"\"\" This notebook reads raw EEG data from EDF files, selects as subset of channels and extracts features for each\n",
    "            of those channels. Data is saved to numpy (.NPY) files for later use. \"\"\")\n",
    "printmd(\"Please note that patient s07 has been removed due to poor quality. The first and last 120s of data have been removed.\")\n",
    "printmd(\"No noise reduction has been applied.\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; &emsp; Feature Extraction Methods (https://www.hindawi.com/journals/isrn/2014/730218/)\n",
    "\n",
    "\n",
    "- Time Frequency Distributions (TFD) <br><br>\n",
    "- Fast Fourier Transform (FFT) <br><br>\n",
    "- Eigenvector Methods (EM), \n",
    "- - Pisarenko’s Method -- Pisarenko Harmonic Decomposition --try https://dsp.stackexchange.com/questions/7667/pisarenko-harmonic-decomposition\n",
    "- - MUSIC Method -- https://pypi.org/project/pyargus/\n",
    "- - Minimum Norm Method <br><br>\n",
    "- Wavelet Transform (WT)\n",
    "- - Continuous Wavelet Transform (CWT) Method\n",
    "- - Discrete Wavelet Transform (DWT) <br><br>\n",
    "    \n",
    "- Auto regressive method (ARM)\n",
    "- - Yule-Walker Method --https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.yule_walker.html\n",
    "- - Burg’s Method <br><br>\n",
    "    \n",
    "More possibilities: https://kourouklides.fandom.com/wiki/Statistical_Signal_Processing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from EEGNET Implementation notebook\n",
    "\n",
    "ignore_list = ['s07']  #list of patient files that should be skipped\n",
    "sample_size = 20\n",
    "\n",
    "import pyedflib\n",
    "mne.set_log_level(\"WARNING\")\n",
    "\n",
    "# get the minimum length of the files\n",
    "def get_minimum_duration(group_directory_name, patient_group_file_prefix):\n",
    "    file_durations = []\n",
    "    for i in range (1, 15): # reading 14 files\n",
    "        patient_id = \"{}{:02d}\".format(patient_group_file_prefix, i)\n",
    "        file_name = '{}\\\\{}.edf'.format(group_directory_name, patient_id)\n",
    "        f = pyedflib.EdfReader(file_name)\n",
    "        file_durations.append(f.file_duration)\n",
    "        f.close()\n",
    "    return(min(file_durations))\n",
    "\n",
    "# modified based on https://stackoverflow.com/a/48704557/2466781\n",
    "def chunk(seq, size):\n",
    "    sl = len(seq) - (len(seq)%size) #exclude values that will be out of range\n",
    "    r = [pd.DataFrame(seq[pos:pos + size]) for pos in range(0, sl, size)]\n",
    "    return r\n",
    "\n",
    "# Modified version of process_patient_group in older notebooks\n",
    "# Uses the raw EDF files and converts to dataframe, dropping the first and last 120 seconds of the shortest  file\n",
    "# All other files are trimmed similarly to produce the same size\n",
    "# Adapted from page 1 of https://buildmedia.readthedocs.org/media/pdf/pyedflib/latest/pyedflib.pdf\n",
    "def process_patient_group(group_directory_name, patient_group_file_prefix, \n",
    "                          minimum_original_duration, \n",
    "                          plot_channels = False,\n",
    "                         channels = ['F8', 'F7', 'F4', 'F3', 'Fz']):\n",
    "    meta_df = pd.DataFrame()\n",
    "    meta = []\n",
    "    patient_id_list = []\n",
    "    for i in range (1, 15): # reading 14 files\n",
    "        patient_id = \"{}{:02d}\".format(patient_group_file_prefix, i)\n",
    "        patient_id_list.append(patient_id)\n",
    "        \n",
    "        file_name = '{}\\\\{}.edf'.format(group_directory_name, patient_id)\n",
    "        data = mne.io.read_raw_edf(file_name)\n",
    "        df = data.to_data_frame()\n",
    "        df2 = df[channels]\n",
    "        ## drop the first 120 seconds and last 120 seconds\n",
    "        df2 = df2[120: (minimum_original_duration-120)]\n",
    "        #f = pyedflib.EdfReader(file_name)\n",
    "        #f.close()\n",
    "        if patient_id not in ignore_list:\n",
    "            meta_df = meta_df.append(df2)\n",
    "            \n",
    "    batches = chunk(meta_df, sample_size)\n",
    "\n",
    "    for batch in batches:\n",
    "        #display(np.asarray(batch.values).shape)\n",
    "        meta.append([np.asarray(batch.values)])\n",
    "           \n",
    "                    \n",
    "    return meta\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving raw data\n",
      "Minimum file duration: 740 seconds\n",
      "Healthy Controls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(350, 1, 20, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sz Patients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(325, 1, 20, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve patient data, using a time window determined by the shortest recording\n",
    "# patient s07 is removed\n",
    "\n",
    "all_channels = ['Fp2', 'F8', 'T4', 'T6', 'O2', 'Fp1', 'F7', 'T3', 'T5', 'O1', 'F4',\n",
    "                'C4', 'P4', 'F3', 'C3', 'P3', 'Fz', 'Cz', 'Pz']\n",
    "target_channels = ['T4', 'T6', 'O2', 'T3', 'T5', 'O1',\n",
    "                   'C4', 'P4', 'C3', 'P3', 'Cz', 'Pz']\n",
    "\n",
    "minimum_duration = min(get_minimum_duration(\"Healthy Controls\", \"h\"), get_minimum_duration('SZ Patients', 's'))\n",
    "print('Retrieving raw data')\n",
    "print('Minimum file duration: {} seconds'.format( minimum_duration))\n",
    "print(\"Healthy Controls\")\n",
    "hc_data = process_patient_group('Healthy Controls', 'h', minimum_duration, channels=target_channels)\n",
    "display(np.asarray(hc_data).shape)\n",
    "\n",
    "\n",
    "print('Sz Patients')\n",
    "sz_data = np.asarray(process_patient_group('SZ Patients', 's', minimum_duration, channels=target_channels))\n",
    "display(np.asarray(sz_data).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "%matplotlib inline\n",
    "mne.set_log_level(\"WARNING\")\n",
    "\n",
    "\n",
    "from scipy.fftpack import dct, idct\n",
    "    \n",
    "# adapted from: https://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html#type-iii-dct\n",
    "def fast_fourier_transform(x, dct_type=2):\n",
    "    #norm = 'None' | 'ortho'\n",
    "    # get discreet cosine transforms\n",
    "    fft_dct = dct(dct(x, type=dct_type, norm='ortho'), type=3, norm='ortho')\n",
    "    # get inverse discreet cosine transforms\n",
    "    fft_idct = idct(dct(x, type=dct_type), type=2)\n",
    "    return fft_dct, fft_idct\n",
    "\n",
    "\n",
    "    \n",
    "# Yule-Walker Method -- https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.yule_walker.html\n",
    "# A paper arguing against this method: http://stat.wharton.upenn.edu/~steele/Courses/956/Resource/YWSourceFiles/WhyNotToUseYW.pdf\n",
    "def yule_walker_method(X):\n",
    "    import statsmodels.api as sm\n",
    "    #from statsmodels.datasets.sunspots import load\n",
    "    \n",
    "    #Y = sm.add_constant(X)\n",
    "    #data = load(as_pandas=False)\n",
    "    rho, sigma = sm.regression.yule_walker(X, order=len(X), method=\"mle\")\n",
    "    #return rho: (AR(p) coefficients | and sigma: the estimated residual stdev\n",
    "    return rho, sigma \n",
    "\n",
    "# http://thomas-cokelaer.info/software/spectrum/html/user/ref_param.html\n",
    "# paper with implementation guidelines (no library): https://www.opus-codec.org/docs/vos_fastburg.pdf\n",
    "def burgs_method(X):\n",
    "    from pylab import plot, log10, linspace, axis\n",
    "    #from spectrum import *\n",
    "    AR, P, k = arburg(Y, shape(X)-1) # order must be less than len(Y)\n",
    "    PSD = arma2psd(AR, sides='centerdc')\n",
    "    #plot(linspace(-0.5, 0.5, len(PSD)), 10*log10(PSD/max(PSD)))\n",
    "    #axis([-0.5,0.5,-60,0])\n",
    "    # return the Burg estimates and the Power Spectral Density\n",
    "    return AR, PSD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def music(X,  d=0.5):\n",
    "    import pyargus\n",
    "    import pyargus.directionEstimation as pde\n",
    "    \n",
    "    \n",
    "    #from pyargus.directionEstimation import *\n",
    "    ## R matrix calculation\n",
    "    M=X.shape[0]\n",
    "    N = 2**12  # sample size          \n",
    "    theta = 90 # Incident angle of the test signal\n",
    "    \n",
    "    # Array response vectors of the test signal\n",
    "    a = X #pd.DataFrame(X)#np.exp(np.arange(0,M,1)*1j*2*np.pi*d*np.cos(np.deg2rad(theta)))\n",
    "       \n",
    "    # Generate multichannel test signal \n",
    "    soi = np.random.normal(0,1,N)  # Signal of Interest\n",
    "    soi_matrix  = np.outer( soi, a).T \n",
    "    \n",
    "    # Generate multichannel uncorrelated noise\n",
    "    noise = np.random.normal(0,np.sqrt(10**-10),(M,N))\n",
    "    \n",
    "    # Create received signal\n",
    "    rec_signal = soi_matrix + noise\n",
    "    \n",
    "    ## R matrix calculation\n",
    "    R = pde.corr_matrix_estimate(rec_signal.T, imp=\"mem_eff\")\n",
    "    # Generate scanning vectors\n",
    "    array_alignment = np.arange(0, M, 1) * d\n",
    "    incident_angles= np.arange(0,len(X),1) #modified result set size from static value of 81\n",
    "    ula_scanning_vectors = pde.gen_ula_scanning_vectors(array_alignment, incident_angles)\n",
    "      \n",
    "    # DOA estimation           \n",
    "    #Bartlett= DOA_Bartlett(R, ula_scanning_vectors)    \n",
    "    #Capon = DOA_Capon(R, ula_scanning_vectors)\n",
    "    #MEM = DOA_MEM(R, ula_scanning_vectors)\n",
    "    #LPM = DOA_LPM(R, ula_scanning_vectors, element_select = 0)\n",
    "    MUSIC = pde.DOA_MUSIC(R, ula_scanning_vectors, signal_dimension = 1)\n",
    "    return MUSIC\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # read data from single sz patient\n",
    "    data = pd.read_csv(\"EEG in Schizophrenia\\\\SZ Patients\\\\s10.csv\")\n",
    "    data.sort_values('Unnamed: 0')\n",
    "    data = data[['s08']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyargus.directionEstimation import *\n",
    "\n",
    "# pisarenko harmonic decomposition\n",
    "# https://tkf.github.io/2010/10/03/estimate-frequency-using-numpy.html\n",
    "\n",
    "PI = np.pi\n",
    "class Pisarenko_HD(object):\n",
    "       \n",
    "\n",
    "\n",
    "    def covariance(self, x, k):\n",
    "        N = len(x) - k\n",
    "        return (x[:-k] * x[k:]).sum() / N\n",
    "\n",
    "\n",
    "    def phd1(self, x):\n",
    "        \"\"\"Estimate frequency using Pisarenko Harmonic Decomposition\"\"\"\n",
    "        r1 = self.covariance(x, 1)\n",
    "        r2 = self.covariance(x, 2)\n",
    "        a = (r2 + np.sqrt(r2 ** 2 + 8 * r1 ** 2)) / 4 / r1\n",
    "        if a > 1:\n",
    "            a = 1\n",
    "        elif a < -1:\n",
    "            a = -1\n",
    "        return np.arccos(a)\n",
    "\n",
    "\n",
    "    def freq(self, x, sample_step=3, dt=1.0):\n",
    "        \"\"\"Estimate frequency using `phd1`\"\"\"\n",
    "        omega = self.phd1(x[::sample_step])\n",
    "        display(x[::sample_step])\n",
    "        display('omega ', omega)\n",
    "        return omega / 2.0 / PI / sample_step / dt\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#https://github.com/takuti/datadog-anomaly-detector/blob/5f3f4d772f344be23fef7986e081db4dcc1029c4/core/changefinder/utils.py#L51-L92\n",
    "def arburg(x, k):\n",
    "    \"\"\"MATLAB implementation of the Burg's method.\n",
    "    cf. https://searchcode.com/codesearch/view/9503568/\n",
    "    \"\"\"\n",
    "\n",
    "    def sumsq(x):\n",
    "        return np.sum(x * x)\n",
    "\n",
    "    n = x.size\n",
    "    # v = sumsq(x)\n",
    "\n",
    "    # f and b are the forward and backward error sequences\n",
    "    f = x[1:n]\n",
    "    b = x[:(n - 1)]\n",
    "\n",
    "    a = np.array([])\n",
    "\n",
    "    # remaining stages i=2 to p\n",
    "    for i in range(k):\n",
    "\n",
    "        # get the i-th reflection coefficient\n",
    "        denominator = sumsq(f) + sumsq(b)\n",
    "        g = 0 if denominator == 0 else 2 * np.sum(f * b) / denominator\n",
    "\n",
    "        # generate next filter order\n",
    "        if i == 0:\n",
    "            a = np.array([g])\n",
    "        else:\n",
    "            a = np.append(g, a - g * a[:(i)][::-1])\n",
    "\n",
    "        # keep track of the error\n",
    "        # v = v * (1 - g * g)\n",
    "\n",
    "        # update the prediction error sequences\n",
    "        old_f = np.empty_like(f)\n",
    "        old_f[:] = f\n",
    "        f = old_f[1:(n - i - 1)] - g * b[1:(n - i - 1)]\n",
    "        b = b[:(n - i - 2)] - g * old_f[:(n - i - 2)]\n",
    "\n",
    "    return -a[:k][::-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wavelet Transforms\n",
    "## paper with nice review of wavelets and wavelet families: https://dergipark.org.tr/en/download/article-file/433655\n",
    "\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# adapted from https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.signal.cwt.html\n",
    "# alternative implementation https://docs.obspy.org/tutorial/code_snippets/continuous_wavelet_transform.html\n",
    "# parameters: X array  - input signal\n",
    "#             show_plot Boolean optional - whether or not to display inline plot of transformed signal\n",
    "#             widths - an array of scales by which to stretch the wavelets -  https://stackoverflow.com/questions/28822327/units-of-widths-argument-to-scipy-signal-cwt-function\n",
    "# returns an array representing the estimate\n",
    "def continuous_wt(X, show_plot=False, widths=None):\n",
    "    if 1 in X.shape :\n",
    "        X = X.flatten()\n",
    "    if widths is None:\n",
    "        widths = np.arange(1, len(X) +1)\n",
    "        widths = np.arange(1, 2)\n",
    "    sig  = np.cos(2 * np.pi * 7 * X) + signal.gausspulse(X - 0.4, fc=2)\n",
    "    cwtmatr = signal.cwt(sig, signal.ricker, widths)\n",
    "    #display('cwtmatr: ', cwtmatr.shape)\n",
    "    if show_plot:\n",
    "        plt.imshow(cwtmatr, extent=[-1, 1, 1, 50], cmap='PRGn', aspect='auto', \n",
    "                   vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n",
    "        plt.show()\n",
    "    return cwtmatr\n",
    "\n",
    "import pywt\n",
    "# source : https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html\n",
    "# parameters: X array wavelet, \n",
    "#             str built-in wavelet name (use wavelist() to get list of all), \n",
    "#             str mode (optional) use pywt.Modes.modes or see https://pywavelets.readthedocs.io/en/latest/ref/signal-extension-modes.html#ref-modes\n",
    "# returns a tuple representing [estimate, coefficients]\n",
    "def discrete_wt(X, wavelet_name='db1', mode=None):\n",
    "    if mode is None:\n",
    "        r = pywt.dwt(X, wavelet_name)\n",
    "    else:\n",
    "        r = pywt.dwt(X, wavelet_name, mode=mode)\n",
    "    return r\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy==1.2 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def ordinary_least_squares(X):\n",
    "    #sm.OLS(spector_data.endog, spector_data.exog)\n",
    "    print(len(X))\n",
    "    Y = list(range(0, len(X)))\n",
    "    #print(len(Y))\n",
    "    #s = sm.regression.recursive_ls(Y, X)#Poisson.OLS(Y,X)\n",
    "    mod = sm.OLS(Y, X)\n",
    "    \n",
    "    res = mod.fit()\n",
    "    s = res.params\n",
    "    \n",
    "    return s\n",
    "#ordinary_least_squares(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\marit\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (1.16.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (42.0.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (0.14.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (3.0.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (0.25.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from mlxtend) (0.22.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\marit\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2018.7)\n",
      "Requirement already satisfied: six in c:\\users\\marit\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlxtend.feature_extraction.linear_discriminant_analysis.LinearDiscriminantAnalysis at 0x28ede516f28>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlxtend\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.feature_extraction import LinearDiscriminantAnalysis\n",
    "from mlxtend.preprocessing import standardize\n",
    "\n",
    "X, y = iris_data()\n",
    "\n",
    "#display(X[:10])\n",
    "#display(y[:10])\n",
    "from pylab import linspace\n",
    "#X = linspace(-1, 1, 200)\n",
    "#X =  X[:, np.newaxis]\n",
    "#y = np.asarray(range(0, 200))\n",
    "#print('X.shape', X.shape)\n",
    "#print('y.shape', y.shape)\n",
    "X = standardize(X)\n",
    "\n",
    "\n",
    "## this is for feature projection, and reduces the dimensionality\n",
    "lda = LinearDiscriminantAnalysis(n_discriminants=2)\n",
    "lda.fit(X, y)\n",
    "#X_lda = lda.transform(X)\n",
    "#display(X_lda[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pywt\n",
    "\n",
    "test = linspace(-1, 1, 200, endpoint=False)\n",
    "\n",
    "# for QMF and Orthogonal Filter Bank, may need to use a filtered array as input - docs here : https://pywavelets.readthedocs.io/en/latest/ref/other-functions.html#central-frequency-of-psi-wavelet-function\n",
    "qmf_features = pywt.qmf(test) # Returns the Quadrature Mirror Filter(QMF).\n",
    "ofb_features = pywt.orthogonal_filter_bank(test) # Orthogonal Filter Banks; returns The orthogonal filter bank of the input scaling filter in the order : 1] Decomposition LPF 2] Decomposition HPF 3] Reconstruction LPF 4] Reconstruction HPF\n",
    "#display(ofb_features[3])\n",
    "\n",
    "\n",
    "# inverse stationary wavelet transforms\n",
    "coeffs = pywt.swt(test, 'db2', level=2)\n",
    "iswt_features  = pywt.iswt(coeffs, 'db2')\n",
    "\n",
    "\n",
    "#coeffs = pywt.dwt2(test, 'haar')\n",
    "swt_features = pywt.swt(test, 'haar') #stationary wavelet transforms -- can use 'level' parameter to limit output --using only [0, 2, 4]\n",
    "swt_features = [swt_features[0][0], swt_features[1][0], swt_features[2][0]]#, swt_features[4])\n",
    "#qmf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marit\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:538: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#takes nd arrays of patient data and creates array of extracted features for use in CNN and machine learning\n",
    "def extract_features(patient_data):\n",
    "    all_features = []\n",
    "    for entry in patient_data:\n",
    "        sample_features = []\n",
    "        # transpose the array so that each row represents a channel (instead of a slice of time)\n",
    "        entry_t = entry[0].transpose() \n",
    "        for channel_data in entry_t:\n",
    "            t = channel_data\n",
    "            fft, inverse_fft = fast_fourier_transform(t)\n",
    "            ywm, ywm_res_stdev = yule_walker_method(t)\n",
    "            pisarenko_hd = Pisarenko_HD()\n",
    "            music_features = music(t)\n",
    "            burgs = arburg(t, len(t))\n",
    "            cwt = continuous_wt(t)\n",
    "            dwt = discrete_wt(t)\n",
    "             # pad dwt values with zeros so that size matches with other feature vectors\n",
    "            dwt = list(dwt[0]) + list(np.zeros(len(dwt[0]))) \n",
    "           \n",
    "            ##qmf_features = pywt.qmf(t) # Returns the Quadrature Mirror Filter(QMF).\n",
    "            ofb_features = pywt.orthogonal_filter_bank(t) # Orthogonal Filter Banks; returns The orthogonal filter bank of the input scaling filter in the order : 1] Decomposition LPF 2] Decomposition HPF 3] Reconstruction LPF 4] Reconstruction HPF\n",
    "\n",
    "            # inverse stationary wavelet transforms\n",
    "            coeffs = pywt.swt(t, 'db2', level=2)\n",
    "            ##iswt_features  = pywt.iswt(coeffs, 'db2')\n",
    "            \n",
    "\n",
    "            ##coeffs = pywt.dwt2(test, 'haar')\n",
    "            ##swt_features = pywt.swt(t, 'haar') #stationary wavelet transforms -- can use 'level' parameter to limit output --using only [0, 2, 4]\n",
    "            #swt_f = [swt_features[0][0], swt_features[1][0], swt_features[2][0]]#, swt_features[4])\n",
    "            sf = np.asarray([fft, inverse_fft, ywm, music_features, burgs, cwt[0], dwt, \n",
    "                            ## qmf_features, iswt_features, ofb_features[0], ofb_features[1], ofb_features[2], ofb_features[3],\n",
    "                            ## swt_features[0][0], swt_features[0][1], swt_features[1][0], swt_features[1][1]\n",
    "                            ], dtype=np.float32)   \n",
    "            sample_features.extend([sf])\n",
    "        all_features.append(np.asarray(sample_features, dtype=np.float32))\n",
    "    return all_features\n",
    "\n",
    "\n",
    "hc_features = extract_features(hc_data)\n",
    "sz_features = extract_features(sz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 12, 7, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape\n",
    "np.asarray(hc_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ndarrays to .npy files\n",
    "def save_ndarray(x, file_name):\n",
    "    np.save(file_name, x)\n",
    "    \n",
    "save_ndarray(hc_features, 'hc_features')\n",
    "save_ndarray(sz_features, 'sz_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing environment settings\n",
      "\n",
      "Python version:  3.7.1\n",
      "\n",
      "Installed modules:\n",
      "\n",
      "absl-py==0.7.1\n",
      "alabaster==0.7.12\n",
      "altgraph==0.16.1\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.6\n",
      "anaconda-project==0.8.2\n",
      "asn1crypto==0.24.0\n",
      "astor==0.8.0\n",
      "astroid==2.1.0\n",
      "astropy==3.1\n",
      "atomicwrites==1.2.1\n",
      "attrs==18.2.0\n",
      "Babel==2.6.0\n",
      "backcall==0.1.0\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "beautifulsoup4==4.6.0\n",
      "bitarray==0.8.3\n",
      "bkcharts==0.2\n",
      "blaze==0.11.3\n",
      "bleach==3.0.2\n",
      "blis==0.2.4\n",
      "bokeh==1.0.2\n",
      "boto==2.49.0\n",
      "boto3==1.9.169\n",
      "botocore==1.12.169\n",
      "Bottleneck==1.2.1\n",
      "cachetools==3.1.1\n",
      "certifi==2018.11.29\n",
      "cffi==1.11.5\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==0.6.1\n",
      "clyent==1.2.2\n",
      "colorama==0.4.1\n",
      "colorlover==0.3.0\n",
      "comtypes==1.1.7\n",
      "conda==4.5.12\n",
      "conda-build==3.17.6\n",
      "conda-verify==3.1.1\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.4.2\n",
      "cufflinks==0.15\n",
      "cycler==0.10.0\n",
      "cymem==2.0.2\n",
      "Cython==0.29.2\n",
      "cytoolz==0.9.0.1\n",
      "dask==1.0.0\n",
      "datashape==0.5.4\n",
      "decorator==4.3.0\n",
      "defusedxml==0.5.0\n",
      "distributed==1.25.1\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.0.2\n",
      "filelock==3.0.10\n",
      "Flask==1.0.2\n",
      "Flask-Cors==3.0.7\n",
      "future==0.17.1\n",
      "gast==0.2.2\n",
      "genderdecoder==0.3\n",
      "gensim==3.7.3\n",
      "gevent==1.3.7\n",
      "glob2==0.6\n",
      "google-auth==1.8.2\n",
      "google-auth-oauthlib==0.4.1\n",
      "google-pasta==0.1.8\n",
      "graphviz==0.13.2\n",
      "greenlet==0.4.15\n",
      "grpcio==1.21.1\n",
      "h5py==2.8.0\n",
      "heapdict==1.0.0\n",
      "html2text==2018.1.9\n",
      "html5lib==1.0.1\n",
      "idna==2.8\n",
      "imageio==2.4.1\n",
      "imagesize==1.1.0\n",
      "importlib-metadata==0.6\n",
      "inflection==0.3.1\n",
      "ipykernel==5.1.0\n",
      "ipython==7.2.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "isort==4.3.4\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4\n",
      "jedi==0.13.2\n",
      "Jinja2==2.10\n",
      "jmespath==0.9.4\n",
      "joblib==0.14.1\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.4\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.35.3\n",
      "jupyterlab-server==0.2.0\n",
      "Keras==2.2.4\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.0.9\n",
      "keyring==17.0.0\n",
      "kiwisolver==1.0.1\n",
      "lazy-object-proxy==1.3.1\n",
      "libarchive-c==2.8\n",
      "llvmlite==0.26.0\n",
      "locket==0.2.0\n",
      "lxml==4.2.5\n",
      "Markdown==3.0.1\n",
      "MarkupSafe==1.1.0\n",
      "matplotlib==3.0.2\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.14\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.6\n",
      "mkl-random==1.0.2\n",
      "mlxtend==0.17.0\n",
      "mne==0.19.2\n",
      "mock==3.0.5\n",
      "more-itertools==4.3.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.5.6\n",
      "multipledispatch==0.6.0\n",
      "murmurhash==1.0.2\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.4.0\n",
      "nbformat==4.4.0\n",
      "ndg-httpsclient==0.5.1\n",
      "networkx==2.2\n",
      "nltk==3.4.1\n",
      "nose==1.3.7\n",
      "notebook==5.7.4\n",
      "numba==0.41.0\n",
      "numexpr==2.6.8\n",
      "numpy==1.16.5\n",
      "numpydoc==0.8.0\n",
      "oauthlib==3.1.0\n",
      "odo==0.5.1\n",
      "olefile==0.46\n",
      "openpyxl==2.5.12\n",
      "opt-einsum==3.1.0\n",
      "packaging==18.0\n",
      "pandas==0.25.3\n",
      "pandocfilters==1.4.2\n",
      "parso==0.3.1\n",
      "partd==0.3.9\n",
      "path.py==11.5.0\n",
      "pathlib2==2.3.3\n",
      "patsy==0.5.1\n",
      "pefile==2019.4.18\n",
      "pep8==1.7.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==5.3.0\n",
      "pkginfo==1.4.2\n",
      "plac==0.9.6\n",
      "plotly==3.10.0\n",
      "pluggy==0.8.0\n",
      "ply==3.11\n",
      "preshed==2.0.1\n",
      "prometheus-client==0.5.0\n",
      "prompt-toolkit==2.0.7\n",
      "protobuf==3.7.1\n",
      "psutil==5.4.8\n",
      "pupygrib==0.5.0\n",
      "py==1.7.0\n",
      "pyargus==1.0.post3\n",
      "pyasn1==0.4.7\n",
      "pyasn1-modules==0.2.7\n",
      "pycodestyle==2.4.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.2\n",
      "pydot==1.4.1\n",
      "pyEDFlib==0.1.15\n",
      "pyflakes==2.0.0\n",
      "Pygments==2.3.1\n",
      "PyInstaller==4.0.dev0+1eadfa55f2\n",
      "pylint==2.2.2\n",
      "pymssql==2.1.4\n",
      "PyMySQL==0.9.3\n",
      "pyodbc==4.0.25\n",
      "pyOpenSSL==18.0.0\n",
      "pyparsing==2.3.0\n",
      "Pyphen==0.9.5\n",
      "pyproj==2.4.2.post1\n",
      "pyriemann==0.2.5\n",
      "PySocks==1.6.8\n",
      "pytest==4.0.2\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.5.0\n",
      "pytest-doctestplus==0.2.0\n",
      "pytest-openfiles==0.3.1\n",
      "pytest-remotedata==0.3.1\n",
      "python-dateutil==2.7.5\n",
      "pytz==2018.7\n",
      "PyWavelets==1.0.1\n",
      "pywin32==223\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==0.5.5\n",
      "PyYAML==3.13\n",
      "pyzmq==17.1.2\n",
      "QtAwesome==0.5.3\n",
      "qtconsole==4.4.3\n",
      "QtPy==1.5.2\n",
      "Quandl==3.4.8\n",
      "repoze.lru==0.7\n",
      "requests==2.21.0\n",
      "requests-oauthlib==1.3.0\n",
      "retrying==1.3.3\n",
      "rope==0.11.0\n",
      "rsa==4.0\n",
      "ruamel-yaml==0.15.46\n",
      "s3transfer==0.2.1\n",
      "scikit-image==0.14.1\n",
      "scikit-learn==0.22.1\n",
      "scipy==1.4.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.12.0\n",
      "smart-open==1.8.4\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==1.0.1\n",
      "sortedcontainers==2.1.0\n",
      "spacy==2.1.4\n",
      "Sphinx==1.8.2\n",
      "sphinxcontrib-websupport==1.1.0\n",
      "spyder==3.3.2\n",
      "spyder-kernels==0.3.0\n",
      "SQLAlchemy==1.2.15\n",
      "srsly==0.0.5\n",
      "statsmodels==0.9.0\n",
      "style==1.1.0\n",
      "sympy==1.3\n",
      "tables==3.4.4\n",
      "tblib==1.3.2\n",
      "tensorboard==1.13.1\n",
      "tensorflow==1.13.1\n",
      "tensorflow-estimator==1.13.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.4.2\n",
      "textblob==0.15.3\n",
      "textstat==0.5.6\n",
      "thinc==7.0.4\n",
      "toolz==0.9.0\n",
      "tornado==5.1.1\n",
      "tqdm==4.28.1\n",
      "traitlets==4.3.2\n",
      "unicodecsv==0.14.1\n",
      "update==0.0.1\n",
      "urllib3==1.24.1\n",
      "wasabi==0.2.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.4.2\n",
      "win-inet-pton==1.0.1\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt==1.11.2\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.1.2\n",
      "xlwings==0.15.1\n",
      "xlwt==1.3.0\n",
      "zict==0.1.3\n"
     ]
    }
   ],
   "source": [
    "## End of implementation code  \n",
    "\n",
    "print('Printing environment settings')\n",
    "\n",
    "from platform import python_version\n",
    "print('\\nPython version: ', python_version())\n",
    "print('\\nInstalled modules:\\n')\n",
    "\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
