{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marit\\Anaconda3\\lib\\site-packages\\numba\\decorators.py:146: RuntimeWarning: Caching is not available when the 'parallel' target is in use. Caching is now being disabled to allow execution to continue.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# edf libraries\n",
    "import pyedflib\n",
    "import mne\n",
    "\n",
    "\n",
    "#pca dependencies\n",
    "from sklearn import decomposition\n",
    "\n",
    "#direction estimation class dependencies\n",
    "import pyargus\n",
    "import pyargus.directionEstimation as pde\n",
    "from pyargus.directionEstimation import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines){\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Feature Extraction from EEG Data - Output to .NPY files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " This notebook reads raw EEG data from EDF files, selects as subset of channels and extracts features for each\n",
       "            of those channels. Data is saved to numpy (.NPY) files for later use. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please note that patient s07 has been removed due to poor quality. The first and last 120s of data have been removed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Principal Component Analysis is used as the primary method of reducing noise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(\"# Feature Extraction from EEG Data - Output to .NPY files\")\n",
    "printmd(\"\"\" This notebook reads raw EEG data from EDF files, selects as subset of channels and extracts features for each\n",
    "            of those channels. Data is saved to numpy (.NPY) files for later use. \"\"\")\n",
    "printmd(\"Please note that patient s07 has been removed due to poor quality. The first and last 120s of data have been removed.\")\n",
    "printmd(\"Principal Component Analysis is used as the primary method of reducing noise.\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; &emsp; Feature Extraction Methods (https://www.hindawi.com/journals/isrn/2014/730218/)\n",
    "\n",
    "\n",
    "- Time Frequency Distributions (TFD) <br><br>\n",
    "- Fast Fourier Transform (FFT) <br><br>\n",
    "- Eigenvector Methods (EM), \n",
    "- - Pisarenko’s Method -- Pisarenko Harmonic Decomposition --try https://dsp.stackexchange.com/questions/7667/pisarenko-harmonic-decomposition\n",
    "- - MUSIC Method -- https://pypi.org/project/pyargus/\n",
    "- - Minimum Norm Method <br><br>\n",
    "- Wavelet Transform (WT)\n",
    "- - Continuous Wavelet Transform (CWT) Method\n",
    "- - Discrete Wavelet Transform (DWT) <br><br>\n",
    "    \n",
    "- Auto regressive method (ARM)\n",
    "- - Yule-Walker Method --https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.yule_walker.html\n",
    "- - Burg’s Method <br><br>\n",
    "    \n",
    "More possibilities: https://kourouklides.fandom.com/wiki/Statistical_Signal_Processing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from EEGNET Implementation notebook\n",
    "\n",
    "ignore_list = ['s07']  #list of patient files that should be skipped\n",
    "sample_size = 20\n",
    "\n",
    "import pyedflib\n",
    "mne.set_log_level(\"WARNING\")\n",
    "\n",
    "# get the minimum length of the files\n",
    "def get_minimum_duration(group_directory_name, patient_group_file_prefix):\n",
    "    file_durations = []\n",
    "    for i in range (1, 15): # reading 14 files\n",
    "        patient_id = \"{}{:02d}\".format(patient_group_file_prefix, i)\n",
    "        file_name = '{}\\\\{}.edf'.format(group_directory_name, patient_id)\n",
    "        f = pyedflib.EdfReader(file_name)\n",
    "        file_durations.append(f.file_duration)\n",
    "        f.close()\n",
    "    return(min(file_durations))\n",
    "\n",
    "# modified based on https://stackoverflow.com/a/48704557/2466781\n",
    "def chunk(seq, size):\n",
    "    sl = len(seq) - (len(seq)%size) #exclude values that will be out of range\n",
    "    r = [pd.DataFrame(seq[pos:pos + size]) for pos in range(0, sl, size)]\n",
    "    return r\n",
    "\n",
    "# Modified version of process_patient_group in older notebooks\n",
    "# Uses the raw EDF files and converts to dataframe, dropping the first and last 120 seconds of the shortest  file\n",
    "# All other files are trimmed similarly to produce the same size\n",
    "# Adapted from page 1 of https://buildmedia.readthedocs.org/media/pdf/pyedflib/latest/pyedflib.pdf\n",
    "def process_patient_group(group_directory_name, patient_group_file_prefix, \n",
    "                          minimum_original_duration, \n",
    "                          plot_channels = False,\n",
    "                         channels = ['F8', 'F7', 'F4', 'F3', 'Fz']):\n",
    "    meta_df = pd.DataFrame()\n",
    "    meta = []\n",
    "    patient_id_list = []\n",
    "    for i in range (1, 15): # reading 14 files\n",
    "        patient_id = \"{}{:02d}\".format(patient_group_file_prefix, i)\n",
    "        patient_id_list.append(patient_id)\n",
    "        \n",
    "        file_name = '{}\\\\{}.edf'.format(group_directory_name, patient_id)\n",
    "        data = mne.io.read_raw_edf(file_name)\n",
    "        df = data.to_data_frame()\n",
    "        df2 = df[channels]\n",
    "        ## drop the first 120 seconds and last 120 seconds\n",
    "        df2 = df2[120: (minimum_original_duration-120)]\n",
    "        #f = pyedflib.EdfReader(file_name)\n",
    "        #f.close()\n",
    "        if patient_id not in ignore_list:\n",
    "            meta_df = meta_df.append(df2)\n",
    "            \n",
    "    batches = chunk(meta_df, sample_size)\n",
    "\n",
    "    for batch in batches:\n",
    "        #display(np.asarray(batch.values).shape)\n",
    "        meta.append([np.asarray(batch.values,dtype=np.float32)])\n",
    "           \n",
    "                    \n",
    "    return meta\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving raw data\n",
      "Minimum file duration: 740 seconds\n",
      "Healthy Controls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(350, 1, 20, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sz Patients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(325, 1, 20, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve patient data, using a time window determined by the shortest recording\n",
    "# patient s07 is removed\n",
    "\n",
    "all_channels = ['Fp2', 'F8', 'T4', 'T6', 'O2', 'Fp1', 'F7', 'T3', 'T5', 'O1', 'F4',\n",
    "                'C4', 'P4', 'F3', 'C3', 'P3', 'Fz', 'Cz', 'Pz']\n",
    "target_channels = ['T4', 'T6', 'O2', 'T3', 'T5', 'O1',\n",
    "                   'C4', 'P4', 'C3', 'P3', 'Cz', 'Pz']\n",
    "\n",
    "minimum_duration = min(get_minimum_duration(\"Healthy Controls\", \"h\"), get_minimum_duration('SZ Patients', 's'))\n",
    "print('Retrieving raw data')\n",
    "print('Minimum file duration: {} seconds'.format( minimum_duration))\n",
    "print(\"Healthy Controls\")\n",
    "hc_data = process_patient_group('Healthy Controls', 'h', minimum_duration, channels=target_channels)\n",
    "display(np.asarray(hc_data).shape)\n",
    "\n",
    "\n",
    "print('Sz Patients')\n",
    "sz_data = np.asarray(process_patient_group('SZ Patients', 's', minimum_duration, channels=target_channels), dtype=np.float32)\n",
    "display(np.asarray(sz_data).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "%matplotlib inline\n",
    "mne.set_log_level(\"WARNING\")\n",
    "\n",
    "\n",
    "from scipy.fftpack import dct, idct\n",
    "    \n",
    "# adapted from: https://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html#type-iii-dct\n",
    "def fast_fourier_transform(x, dct_type=2):\n",
    "    #norm = 'None' | 'ortho'\n",
    "    # get discreet cosine transforms\n",
    "    fft_dct = dct(dct(x, type=dct_type, norm='ortho'), type=3, norm='ortho')\n",
    "    # get inverse discreet cosine transforms\n",
    "    fft_idct = idct(dct(x, type=dct_type), type=2)\n",
    "    return fft_dct, fft_idct\n",
    "\n",
    "\n",
    "    \n",
    "# Yule-Walker Method -- https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.yule_walker.html\n",
    "# A paper arguing against this method: http://stat.wharton.upenn.edu/~steele/Courses/956/Resource/YWSourceFiles/WhyNotToUseYW.pdf\n",
    "def yule_walker_method(X):\n",
    "    import statsmodels.api as sm\n",
    "    #from statsmodels.datasets.sunspots import load\n",
    "    \n",
    "    #Y = sm.add_constant(X)\n",
    "    #data = load(as_pandas=False)\n",
    "    rho, sigma = sm.regression.yule_walker(X, order=len(X), method=\"mle\")\n",
    "    #return rho: (AR(p) coefficients | and sigma: the estimated residual stdev\n",
    "    return rho, sigma \n",
    "\n",
    "# http://thomas-cokelaer.info/software/spectrum/html/user/ref_param.html\n",
    "# paper with implementation guidelines (no library): https://www.opus-codec.org/docs/vos_fastburg.pdf\n",
    "def burgs_method(X):\n",
    "    from pylab import plot, log10, linspace, axis\n",
    "    #from spectrum import *\n",
    "    AR, P, k = arburg(Y, shape(X)-1) # order must be less than len(Y)\n",
    "    PSD = arma2psd(AR, sides='centerdc')\n",
    "    #plot(linspace(-0.5, 0.5, len(PSD)), 10*log10(PSD/max(PSD)))\n",
    "    #axis([-0.5,0.5,-60,0])\n",
    "    # return the Burg estimates and the Power Spectral Density\n",
    "    return AR, PSD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def music(X,  d=0.5):\n",
    "    import pyargus\n",
    "    import pyargus.directionEstimation as pde\n",
    "    \n",
    "    \n",
    "    #from pyargus.directionEstimation import *\n",
    "    ## R matrix calculation\n",
    "    M=X.shape[0]\n",
    "    N = 2**12  # sample size          \n",
    "    theta = 90 # Incident angle of the test signal\n",
    "    \n",
    "    # Array response vectors of the test signal\n",
    "    a = X #pd.DataFrame(X)#np.exp(np.arange(0,M,1)*1j*2*np.pi*d*np.cos(np.deg2rad(theta)))\n",
    "       \n",
    "    # Generate multichannel test signal \n",
    "    soi = np.random.normal(0,1,N)  # Signal of Interest\n",
    "    soi_matrix  = np.outer( soi, a).T \n",
    "    \n",
    "    # Generate multichannel uncorrelated noise\n",
    "    noise = np.random.normal(0,np.sqrt(10**-10),(M,N))\n",
    "    \n",
    "    # Create received signal\n",
    "    rec_signal = soi_matrix + noise\n",
    "    \n",
    "    ## R matrix calculation\n",
    "    R = pde.corr_matrix_estimate(rec_signal.T, imp=\"mem_eff\")\n",
    "    # Generate scanning vectors\n",
    "    array_alignment = np.arange(0, M, 1) * d\n",
    "    incident_angles= np.arange(0,len(X),1) #modified result set size from static value of 81\n",
    "    ula_scanning_vectors = pde.gen_ula_scanning_vectors(array_alignment, incident_angles)\n",
    "      \n",
    "    # DOA estimation           \n",
    "    #Bartlett= DOA_Bartlett(R, ula_scanning_vectors)    \n",
    "    #Capon = DOA_Capon(R, ula_scanning_vectors)\n",
    "    #MEM = DOA_MEM(R, ula_scanning_vectors)\n",
    "    #LPM = DOA_LPM(R, ula_scanning_vectors, element_select = 0)\n",
    "    MUSIC = pde.DOA_MUSIC(R, ula_scanning_vectors, signal_dimension = 1)\n",
    "    return MUSIC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class direction_estimation_methods(object):\n",
    "    def retrieve_features(self, extraction_method, X,  d=0.5):\n",
    "\n",
    "        ## R matrix calculation\n",
    "        M=X.shape[0]\n",
    "        N = 2**12  # sample size          \n",
    "        theta = 90 # Incident angle of the test signal\n",
    "\n",
    "        # Array response vectors of the test signal\n",
    "        a = X #pd.DataFrame(X)#np.exp(np.arange(0,M,1)*1j*2*np.pi*d*np.cos(np.deg2rad(theta)))\n",
    "\n",
    "        # Generate multichannel test signal \n",
    "        soi = np.random.normal(0,1,N)  # Signal of Interest\n",
    "        soi_matrix  = np.outer( soi, a).T \n",
    "\n",
    "        # Generate multichannel uncorrelated noise\n",
    "        noise = np.random.normal(0,np.sqrt(10**-10),(M,N))\n",
    "\n",
    "        # Create received signal\n",
    "        rec_signal = soi_matrix + noise\n",
    "\n",
    "        ## R matrix calculation\n",
    "        R = pde.corr_matrix_estimate(rec_signal.T, imp=\"mem_eff\")\n",
    "        # Generate scanning vectors\n",
    "        array_alignment = np.arange(0, M, 1) * d\n",
    "        incident_angles= np.arange(0,len(X),1) #modified result set size from static value of 81\n",
    "        ula_scanning_vectors = pde.gen_ula_scanning_vectors(array_alignment, incident_angles)\n",
    "\n",
    "        # DOA estimation           \n",
    "        # \n",
    "        \n",
    "        \n",
    "        methods = {\n",
    "            \n",
    "            'Bartlett': DOA_Bartlett(R, ula_scanning_vectors) ,\n",
    "            # Capon is a high resolution nonparametric method used in 3d feature extraction\n",
    "            'Capon': DOA_Capon(R, ula_scanning_vectors),\n",
    "             #Linear prediction model - popular in speech recognition\n",
    "            'LPM': DOA_LPM(R, ula_scanning_vectors, element_select = 0),\n",
    "           \n",
    "            #Burgs maximum entropy model\n",
    "            'MEM': DOA_MEM(R, ula_scanning_vectors),\n",
    "            \n",
    "            'MUSIC': DOA_MUSIC(R, ula_scanning_vectors, signal_dimension = 1)\n",
    "            \n",
    "        }\n",
    "        if extraction_method in methods:\n",
    "            ef = methods[extraction_method]\n",
    "            extracted_features = (ef * np.conj(ef)).astype(np.float32)\n",
    "        else: #return all\n",
    "            ef = methods\n",
    "            for m in methods:\n",
    "                ef[m] =  (ef[m] * np.conj(ef[m])).astype(np.float32)\n",
    "                extracted_features = ef\n",
    "        #multiply complex number by its conjugate to get real number\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "        return extracted_features\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # read data from single sz patient\n",
    "    data = pd.read_csv(\"EEG in Schizophrenia\\\\SZ Patients\\\\s10.csv\")\n",
    "    data.sort_values('Unnamed: 0')\n",
    "    data = data[['s08']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = np.linspace(-1, 1, 30, endpoint=False)\n",
    "#test_output = music(test)\n",
    "#display(test_output)\n",
    "\n",
    "#display((test_output * np.conj(test_output)).astype(np.float32))\n",
    "#(test_output).astype(np.float32)\n",
    "#np.conj(test_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyargus.directionEstimation import *\n",
    "\n",
    "# pisarenko harmonic decomposition\n",
    "# https://tkf.github.io/2010/10/03/estimate-frequency-using-numpy.html\n",
    "\n",
    "PI = np.pi\n",
    "class Pisarenko_HD(object):\n",
    "       \n",
    "\n",
    "\n",
    "    def covariance(self, x, k):\n",
    "        N = len(x) - k\n",
    "        return (x[:-k] * x[k:]).sum() / N\n",
    "\n",
    "\n",
    "    def phd1(self, x):\n",
    "        \"\"\"Estimate frequency using Pisarenko Harmonic Decomposition\"\"\"\n",
    "        r1 = self.covariance(x, 1)\n",
    "        r2 = self.covariance(x, 2)\n",
    "        a = (r2 + np.sqrt(r2 ** 2 + 8 * r1 ** 2)) / 4 / r1\n",
    "        if a > 1:\n",
    "            a = 1\n",
    "        elif a < -1:\n",
    "            a = -1\n",
    "        return np.arccos(a)\n",
    "\n",
    "\n",
    "    def freq(self, x, sample_step=3, dt=1.0):\n",
    "        \"\"\"Estimate frequency using `phd1`\"\"\"\n",
    "        omega = self.phd1(x[::sample_step])\n",
    "        display(x[::sample_step])\n",
    "        display('omega ', omega)\n",
    "        return omega / 2.0 / PI / sample_step / dt\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#https://github.com/takuti/datadog-anomaly-detector/blob/5f3f4d772f344be23fef7986e081db4dcc1029c4/core/changefinder/utils.py#L51-L92\n",
    "def arburg(x, k):\n",
    "    \"\"\"MATLAB implementation of the Burg's method.\n",
    "    cf. https://searchcode.com/codesearch/view/9503568/\n",
    "    \"\"\"\n",
    "\n",
    "    def sumsq(x):\n",
    "        return np.sum(x * x)\n",
    "\n",
    "    n = x.size\n",
    "    # v = sumsq(x)\n",
    "\n",
    "    # f and b are the forward and backward error sequences\n",
    "    f = x[1:n]\n",
    "    b = x[:(n - 1)]\n",
    "\n",
    "    a = np.array([])\n",
    "\n",
    "    # remaining stages i=2 to p\n",
    "    for i in range(k):\n",
    "\n",
    "        # get the i-th reflection coefficient\n",
    "        denominator = sumsq(f) + sumsq(b)\n",
    "        g = 0 if denominator == 0 else 2 * np.sum(f * b) / denominator\n",
    "\n",
    "        # generate next filter order\n",
    "        if i == 0:\n",
    "            a = np.array([g])\n",
    "        else:\n",
    "            a = np.append(g, a - g * a[:(i)][::-1])\n",
    "\n",
    "        # keep track of the error\n",
    "        # v = v * (1 - g * g)\n",
    "\n",
    "        # update the prediction error sequences\n",
    "        old_f = np.empty_like(f)\n",
    "        old_f[:] = f\n",
    "        f = old_f[1:(n - i - 1)] - g * b[1:(n - i - 1)]\n",
    "        b = b[:(n - i - 2)] - g * old_f[:(n - i - 2)]\n",
    "\n",
    "    return -a[:k][::-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wavelet Transforms\n",
    "## paper with nice review of wavelets and wavelet families: https://dergipark.org.tr/en/download/article-file/433655\n",
    "\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# adapted from https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.signal.cwt.html\n",
    "# alternative implementation https://docs.obspy.org/tutorial/code_snippets/continuous_wavelet_transform.html\n",
    "# parameters: X array  - input signal\n",
    "#             show_plot Boolean optional - whether or not to display inline plot of transformed signal\n",
    "#             widths - an array of scales by which to stretch the wavelets -  https://stackoverflow.com/questions/28822327/units-of-widths-argument-to-scipy-signal-cwt-function\n",
    "# returns an array representing the estimate\n",
    "def continuous_wt(X, show_plot=False, widths=None):\n",
    "    if 1 in X.shape :\n",
    "        X = X.flatten()\n",
    "    if widths is None:\n",
    "        widths = np.arange(1, len(X) +1)\n",
    "        widths = np.arange(1, 2)\n",
    "    sig  = np.cos(2 * np.pi * 7 * X) + signal.gausspulse(X - 0.4, fc=2)\n",
    "    cwtmatr = signal.cwt(sig, signal.ricker, widths)\n",
    "    #display('cwtmatr: ', cwtmatr.shape)\n",
    "    if show_plot:\n",
    "        plt.imshow(cwtmatr, extent=[-1, 1, 1, 50], cmap='PRGn', aspect='auto', \n",
    "                   vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n",
    "        plt.show()\n",
    "    return cwtmatr\n",
    "\n",
    "import pywt\n",
    "# source : https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html\n",
    "# parameters: X array wavelet, \n",
    "#             str built-in wavelet name (use wavelist() to get list of all), \n",
    "#             str mode (optional) use pywt.Modes.modes or see https://pywavelets.readthedocs.io/en/latest/ref/signal-extension-modes.html#ref-modes\n",
    "# returns a tuple representing [estimate, coefficients]\n",
    "def discrete_wt(X, wavelet_name='db1', mode=None):\n",
    "    if mode is None:\n",
    "        r = pywt.dwt(X, wavelet_name)\n",
    "    else:\n",
    "        r = pywt.dwt(X, wavelet_name, mode=mode)\n",
    "    return r\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marit\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:49: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def ordinary_least_squares(X):\n",
    "    #sm.OLS(spector_data.endog, spector_data.exog)\n",
    "    print(len(X))\n",
    "    Y = list(range(0, len(X)))\n",
    "    #print(len(Y))\n",
    "    #s = sm.regression.recursive_ls(Y, X)#Poisson.OLS(Y,X)\n",
    "    mod = sm.OLS(Y, X)\n",
    "    \n",
    "    res = mod.fit()\n",
    "    s = res.params\n",
    "    \n",
    "    return s\n",
    "#ordinary_least_squares(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:141: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#takes nd arrays of patient data and creates array of extracted features for use in CNN and machine learning\n",
    "def extract_features(patient_data):\n",
    "    all_features = []\n",
    "    for entry in patient_data:\n",
    "        sample_features = []\n",
    "        # transpose the array so that each row represents a channel (instead of a slice of time)\n",
    "        entry_t = entry[0].transpose() \n",
    "        pca = decomposition.PCA()\n",
    "        #noise reduction\n",
    "        #pca.fit(entry[0])\n",
    "        #denoised_data = pca.transform(entry[0])\n",
    "        data = entry[0]\n",
    "        for channel_data in data:\n",
    "            t = channel_data\n",
    "            #extract features\n",
    "            fft, inverse_fft = fast_fourier_transform(t)\n",
    "            ywm, ywm_res_stdev = yule_walker_method(t)\n",
    "            pisarenko_hd = Pisarenko_HD()\n",
    "            #music_features = (music(t)).astype(float)\n",
    "            dem = direction_estimation_methods()\n",
    "            m = dem.retrieve_features('all', t)\n",
    "            direction_estimation_features = np.asarray([m[n] for n in m])\n",
    "            \n",
    "            music_features = music(t)\n",
    "            #display(music_features)\n",
    "            burgs = arburg(t, len(t))\n",
    "            cwt = continuous_wt(t)\n",
    "            dwt = discrete_wt(t)\n",
    "            # pad dwt values with zeros so that size matches with other feature vectors\n",
    "            dwt = list(dwt[0]) + list(np.zeros(len(dwt[0]))) \n",
    "            #sf = np.asarray([fft, inverse_fft, ywm, burgs, cwt[0], dwt], dtype=np.float32)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            qmf_features = pywt.qmf(t) # Returns the Quadrature Mirror Filter(QMF).\n",
    "            ofb_features = pywt.orthogonal_filter_bank(t) # Orthogonal Filter Banks; returns The orthogonal filter bank of the input scaling filter in the order : 1] Decomposition LPF 2] Decomposition HPF 3] Reconstruction LPF 4] Reconstruction HPF\n",
    "            #display(ofb_features[3])\n",
    "\n",
    "            # inverse stationary wavelet transforms\n",
    "            coeffs = pywt.swt(t, 'db2', level=2)\n",
    "            iswt_features  = pywt.iswt(coeffs, 'db2')\n",
    "\n",
    "            #coeffs = pywt.dwt2(test, 'haar')\n",
    "            swt_features = pywt.swt(t, 'haar') #stationary wavelet transforms -- can use 'level' parameter to limit output --using only [0, 2, 4]\n",
    "            #swt_f = [swt_features[0][0], swt_features[1][0], swt_features[2][0]]#, swt_features[4])\n",
    "            #display(qmf_features)\n",
    "            sf = np.asarray([fft, inverse_fft, ywm, music_features, burgs, cwt[0], dwt, \n",
    "                             qmf_features, iswt_features, ofb_features[0], ofb_features[1], ofb_features[2], ofb_features[3],\n",
    "                             swt_features[0][0], swt_features[0][1], swt_features[1][0], swt_features[1][1]]\n",
    "                            , dtype=np.float32)   \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            sf = np.append(sf, direction_estimation_features, axis=0)\n",
    "            sample_features.extend([sf])\n",
    "        all_features.append(np.asarray(sample_features, dtype=np.float32))\n",
    "    return all_features\n",
    "\n",
    "\n",
    "hc_features = extract_features(hc_data)\n",
    "sz_features = extract_features(sz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 20, 22, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install scipy==1.2 --upgrade\n",
    "np.asarray(hc_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv_test = hc_data[0][0][0]#.reshape(-1, 1)\n",
    "#fv_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "np.linspace(-1, 1, 30, endpoint=False)\n",
    "test = np.linspace(-1, 1, 30, endpoint=False)\n",
    "#music(test)\n",
    "\n",
    "np.asarray(hc_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ndarrays to .npy files\n",
    "def save_ndarray(x, file_name):\n",
    "    np.save(file_name, x)\n",
    "    \n",
    "save_ndarray(hc_features, 'hc_features_22')\n",
    "save_ndarray(sz_features, 'sz_features_22')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of implementation code  \n",
    "\n",
    "print('Printing environment settings')\n",
    "\n",
    "from platform import python_version\n",
    "print('\\nPython version: ', python_version())\n",
    "print('\\nInstalled modules:\\n')\n",
    "\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
